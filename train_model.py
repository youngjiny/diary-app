# train_model.py (Google Sheets ìë™ ì—°ë™ ë²„ì „)

import pandas as pd
import gspread
from google.oauth2.service_account import Credentials
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from soynlp.tokenizer import LTokenizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
import joblib
import os

# â­ï¸ 1. Google Sheetsì—ì„œ ë°ì´í„°ë¥¼ ì§ì ‘ ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜ ì¶”ê°€
def load_data_from_gsheets():
    """Google Sheetsì—ì„œ í”¼ë“œë°± ë°ì´í„°ë¥¼ ì§ì ‘ ì½ì–´ì™€ DataFrameìœ¼ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    print("Google Sheetsì—ì„œ ìµœì‹  ë°ì´í„° ë¡œë”© ì¤‘...")
    try:
        scope = ['https://www.googleapis.com/auth/spreadsheets', 'https://www.googleapis.com/auth/drive']
        # ë¡œì»¬ì— ì €ì¥ëœ ì¸ì¦í‚¤ íŒŒì¼ ì‚¬ìš©
        creds = Credentials.from_service_account_file('gsheets_credentials.json', scopes=scope)
        client = gspread.authorize(creds)
        
        spreadsheet = client.open("diary_app_feedback")
        worksheet = spreadsheet.worksheet("Sheet1")
        
        data = worksheet.get_all_records()
        df = pd.DataFrame(data)
        print("ë°ì´í„° ë¡œë”© ì™„ë£Œ!")
        return df
    except FileNotFoundError:
        print("ì˜¤ë¥˜: 'gsheets_credentials.json' íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        print("1ë‹¨ê³„ ê°€ì´ë“œì— ë”°ë¼ ì¸ì¦í‚¤ íŒŒì¼ì„ í”„ë¡œì íŠ¸ í´ë”ì— ì¤€ë¹„í•´ì£¼ì„¸ìš”.")
        return None
    except Exception as e:
        print(f"Google Sheets ë°ì´í„° ë¡œë”© ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

# --- ëª¨ë¸ í›ˆë ¨ ì‹œì‘ ---
print("--- ëª¨ë¸ í›ˆë ¨ ì‹œì‘ ---")

# â­ï¸ 2. CSVë¥¼ ì½ëŠ” ëŒ€ì‹ , ìœ„ì—ì„œ ë§Œë“  í•¨ìˆ˜ë¥¼ í˜¸ì¶œ
data = load_data_from_gsheets()

# ë°ì´í„° ë¡œë”©ì— ì„±ê³µí–ˆëŠ”ì§€ í™•ì¸
if data is None or data.empty:
    print("ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ì§€ ëª»í•´ í›ˆë ¨ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
    exit()

print(f"ì´ {len(data)}ê°œì˜ ë°ì´í„°ë¡œ í›ˆë ¨ì„ ì‹œì‘í•©ë‹ˆë‹¤.")

# (ì´í•˜ í›ˆë ¨ ë¡œì§ì€ ì´ì „ê³¼ ë™ì¼)
if len(data) < 50:
    X_train = data['text']
    y_train = data['label']
    print("ë°ì´í„° ì–‘ì´ ì ì–´, ì „ì²´ ë°ì´í„°ë¥¼ í›ˆë ¨ì— ì‚¬ìš©í•©ë‹ˆë‹¤.")
else:
    X = data['text']
    y = data['label']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

tokenizer = LTokenizer()
tfidf_vectorizer = TfidfVectorizer(tokenizer=tokenizer.tokenize, min_df=1, ngram_range=(1, 2))
X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)

model = LogisticRegression(random_state=42)
model.fit(X_train_tfidf, y_train)

if len(data) >= 50:
    X_test_tfidf = tfidf_vectorizer.transform(X_test)
    y_pred = model.predict(X_test_tfidf)
    accuracy = accuracy_score(y_test, y_pred)
    print(f"âœ… ëª¨ë¸ ì •í™•ë„: {accuracy:.4f}")

joblib.dump(model, 'sentiment_model.pkl')
joblib.dump(tfidf_vectorizer, 'tfidf_vectorizer.pkl')

print("--- í›ˆë ¨ ì™„ë£Œ! ---")
print("ğŸ“„ sentiment_model.pkl (ëª¨ë¸) ì™€ ğŸ“„ tfidf_vectorizer.pkl (ë²¡í„°ë¼ì´ì €) íŒŒì¼ì´ ìƒˆë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")